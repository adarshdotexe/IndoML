{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Clone this repository:\n",
    "!git clone https://github.com/adarshdotexe/IndoML.git\n",
    "%cd /content/IndoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the requirements:\n",
    "%pip install torch transformers datasets sentencepiece rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries:\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import device\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data:\n",
    "with open('train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "utt = []\n",
    "intent = []\n",
    "classes = []\n",
    "for i in data:\n",
    "    utt.extend(i['utt'])\n",
    "    intent.extend([i['intent'] for _ in i['utt']])\n",
    "    classes.append(i['intent'])\n",
    "\n",
    "# Shuffle the data:\n",
    "random.seed(42)\n",
    "arr = [i for i in range(len(utt))]\n",
    "random.shuffle(arr)\n",
    "utt = [utt[i] for i in arr]\n",
    "intent = [intent[i] for i in arr]\n",
    "\n",
    "# Split the data into train and test sets:\n",
    "train_utt, test_utt, train_intent, test_intent = train_test_split(utt, intent, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model:\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EvalPrediction\n",
    "model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform zero-shot classification:\n",
    "sequence = \"This is a classification problem.\"\n",
    "hypothesis = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "batch = tokenizer(sequence, hypothesis, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "logits = model(**batch).logits\n",
    "\n",
    "# Now we need to convert the data into the format that the model can understand:\n",
    "\n",
    "\n",
    "for i in range(len(utt)):\n",
    "    premise = utt[i]\n",
    "    hypothesis = intent[i]\n",
    "    seq = premise + tokenizer.sep_token + f\"This is {hypothesis}\"\n",
    "    label = \"entailment\"\n",
    "    utt[i] = seq\n",
    "    intent[i] = label\n",
    "\n",
    "\n",
    "# Split the data into train and test:\n",
    "train_utt, test_utt, train_intent, test_intent = train_test_split(utt, intent, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Tokenize the data:\n",
    "train_encodings = tokenizer(train_utt, truncation = True, padding = True)\n",
    "test_encodings = tokenizer(test_utt, truncation = True, padding = True)\n",
    "\n",
    "# Convert the data into a format that the model can understand:\n",
    "\n",
    "class IndoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "train_dataset = IndoDataset(train_encodings, train_intent)\n",
    "\n",
    "test_dataset = IndoDataset(test_encodings, test_intent)\n",
    "\n",
    "# Define the training arguments:\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=20,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        save_strategy='epoch',\n",
    "        save_steps=20\n",
    ")\n",
    "\n",
    "# Define the compute metrics function:\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "  metric_acc = load_metric(\"accuracy\")\n",
    "  metric_f1 = load_metric(\"f1\")\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  preds = np.argmax(preds, axis = 1)\n",
    "  result = {}\n",
    "  result[\"accuracy\"] = metric_acc.compute(predictions = preds, references = p.label_ids)[\"accuracy\"]\n",
    "  result[\"f1\"] = metric_f1.compute(predictions = preds, references = p.label_ids, average = 'macro')[\"f1\"]\n",
    "  return result\n",
    "\n",
    "# Define the trainer:\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use trainer to predict the labels for the massive test set:\n",
    "test = []\n",
    "with open('massive_test.data') as f:\n",
    "    for i in f:\n",
    "        test.append(json.loads(i))\n",
    "result = []\n",
    "for i in test:\n",
    "    seq = []\n",
    "    for j in classes:\n",
    "        seq.append(i['utt'] + tokenizer.sep_token + f\"This is {j}\")\n",
    "    encodings = tokenizer(seq, truncation = True, padding = True)\n",
    "    dataset = IndoDataset(encodings, [0]*len(seq))\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = predictions.predictions[0] if isinstance(predictions.predictions, tuple) else predictions.predictions\n",
    "    preds = [preds[i][0]/(preds[i][0] + preds[i][2]) for i in range(len(preds))]\n",
    "    preds = np.argmax(preds)\n",
    "    # {\"indoml_id\": 5846, \"utt\": \"Could you please order me a new set of golf clubs?\"}\n",
    "    result.append({\"indoml_id\": i['indoml_id'], \"intent\": classes[preds]})\n",
    "    print(i['utt'], classes[preds])\n",
    "\n",
    "with open('massive_test_adarshdotexe', 'w') as f:\n",
    "    for i in result:\n",
    "        f.write(json.dumps(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
